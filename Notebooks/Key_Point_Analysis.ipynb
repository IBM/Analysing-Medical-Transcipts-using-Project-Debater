{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Medical Transcripts with Project Debater \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Project Debater  \n",
    "\n",
    "Project Debater is the first AI system that can debate humans on complex topics. Project Debater digests massive texts, constructs a well-structured speech on a given topic, delivers it with clarity and purpose, and rebuts its opponent. Eventually, Project Debater will help people reason by providing compelling, evidence-based arguments and limiting the influence of emotion, bias, or ambiguity. \n",
    "\n",
    "\n",
    "- In this notebook you will get an insight on how to use Project Debater to analyse and derive insights from medical transcipts.\n",
    "\n",
    "\n",
    "**For prerequisites please refer to this [GitHub Repository](https://github.com/IBM/Analysing-Medical-Transcipts-using-Project-Debater)**\n",
    "\n",
    "**Please also make sure to use this script with helper functions [austin_utils.py](https://github.ibm.com/TechnologyGarageUKI/Project-Debater/blob/master/Code/austin_utils.py)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "**The data that you will explore in this notebook contains sample medical transcriptions for various medical specialities.**\n",
    "\n",
    "You can download this data directly [using this link](https://www.kaggle.com/tboyle10/medicaltranscriptions) \n",
    "\n",
    "**Let's start with importing the required Python packages and loading our data into the notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Set Api-Key:')\n",
    "api_key = ''\n",
    "\n",
    "print('Install Early-Access-Program SDK:')\n",
    "!wget -P . https://early-access-program.debater.res.ibm.com/sdk/python_api.tar.gz\n",
    "!tar -xvf python_api.tar.gz\n",
    "!cd python_api ; pip install .\n",
    "!rm -f python_api.tar.gz*\n",
    "\n",
    "print('Retrieve datset and additional code from the Github repo: https://github.com/IBM/Analysing-Medical-Transcipts-using-Project-Debater :') \n",
    "!rm -f mtsamples_descriptions_clean*\n",
    "!rm -f austin_utils*\n",
    "\n",
    "\n",
    "!wget -P . https://raw.githubusercontent.com/IBM/Analysing-Medical-Transcipts-using-Project-Debater/main/Data/mtsamples_descriptions_clean.csv\n",
    "!wget -P . https://raw.githubusercontent.com/IBM/Analysing-Medical-Transcipts-using-Project-Debater/main/Data/austin_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import csv\n",
    "import plotly.express as px\n",
    "import urllib.request\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "with open('./mtsamples_descriptions_clean.csv') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    sentences = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d sentences in the dataset' % len(sentences))\n",
    "print('Each sentence is a dictionary with the following keys: %s' % str(sentences[0].keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viz_sentences = pd.DataFrame(sentences)\n",
    "viz_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialise Debater client**\n",
    "\n",
    "The Key Point Analysis service stores the data (and results cache) in a domain. A user can create several domains, one for each dataset. Domains are only accessible to the user who created them.\n",
    "\n",
    "Full documentation of the Key Point Analysis service can be found [here](https://early-access-program.debater.res.ibm.com/docs/services/keypoints/keypoints_pydoc.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise client\n",
    "from debater_python_api.api.debater_api import DebaterApi\n",
    "from austin_utils import init_logger\n",
    "# import os\n",
    "\n",
    "init_logger()\n",
    "api_key = ''\n",
    "debater_api = DebaterApi(apikey=api_key)\n",
    "keypoints_client = debater_api.get_keypoints_client()\n",
    "\n",
    "domain = 'medical_demo'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select top 1000 sentences from data using _Argument Quality_ service**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from austin_utils import print_top_and_bottom_k_sentences\n",
    "\n",
    "def get_top_quality_sentences(sentences, top_k, topic):    \n",
    "    arg_quality_client = debater_api.get_argument_quality_client()\n",
    "    sentences_topic = [{'sentence': sentence['text'], 'topic': topic} for sentence in sentences]\n",
    "    arg_quality_scores = arg_quality_client.run(sentences_topic)\n",
    "    sentences_and_scores = zip(sentences, arg_quality_scores)\n",
    "    sentences_and_scores_sorted = sorted(sentences_and_scores, key=lambda x: x[1], reverse=True)\n",
    "    sentences_sorted = [sentence for sentence, _ in sentences_and_scores_sorted]\n",
    "    print_top_and_bottom_k_sentences(sentences_sorted, 10)\n",
    "    return sentences_sorted[:top_k]\n",
    "\n",
    "sentences_top_1000_aq = get_top_quality_sentences(sentences, 1000, \n",
    "                            'The patient is a 30-year-old who was admitted with symptoms including obstructions, failures and pain that started four days ago.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kpa(sentences, run_params):\n",
    "    sentences_texts = [sentence['text'] for sentence in sentences]\n",
    "    sentences_ids = [sentence['id'] for sentence in sentences]\n",
    "\n",
    "    keypoints_client.delete_domain_cannot_be_undone(domain) # Clear domain in case it existed already\n",
    "\n",
    "    keypoints_client.upload_comments(domain=domain, \n",
    "                                     comments_ids=sentences_ids, \n",
    "                                     comments_texts=sentences_texts, \n",
    "                                     dont_split=True)\n",
    "\n",
    "    keypoints_client.wait_till_all_comments_are_processed(domain=domain)\n",
    "\n",
    "    future = keypoints_client.start_kp_analysis_job(domain=domain, \n",
    "                                                    comments_ids=sentences_ids, \n",
    "                                                    run_params=run_params)\n",
    "\n",
    "    kpa_result = future.get_result(high_verbosity=False, \n",
    "                                   polling_timout_secs=5)\n",
    "    \n",
    "    return kpa_result, future.get_job_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **mapping_threshold**  (Float in [0.0,1.0], set to 0.99 by default): The matching threshold, scores above are considered a match. A higher threshold leads to a higher precision and a lower coverage.\n",
    "* **n_top_kps** (Integer, default is set by an internal algorithm): Number of key points to generate. Lower value will make the job finish faster. All sentences are re-mapped to these key point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from austin_utils import print_results\n",
    "\n",
    "kpa_result, _ = run_kpa(sentences_top_1000_aq, {'n_top_kps': 20,\n",
    "                                                'mapping_threshold': 0.95})\n",
    "# print_results(kpa_result, n_sentences_per_kp=2, title='Top 1000 sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from austin_utils import print_results_in_a_table\n",
    "print_results_in_a_table(kpa_result, n_sentences_per_kp=5, title='Top 1000 sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export results to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_to_df(result):  \n",
    "    matchings_rows = []\n",
    "    for keypoint_matching in result['keypoint_matchings']:\n",
    "        kp = keypoint_matching['keypoint']\n",
    "        for match in keypoint_matching['matching']:\n",
    "            match_row = [kp, match[\"sentence_text\"], match[\"score\"], match[\"comment_id\"], match[\"sentence_id\"],\n",
    "                            match[\"sents_in_comment\"], match[\"span_start\"], match[\"span_end\"], match[\"num_tokens\"],\n",
    "                            match[\"argument_quality\"]]\n",
    "\n",
    "            matchings_rows.append(match_row)\n",
    "\n",
    "    cols = [\"kp\", \"sentence_text\", \"match_score\", 'comment_id', 'sentence_id', 'sents_in_comment', 'span_start',\n",
    "            'span_end', 'num_tokens', 'argument_quality']\n",
    "    match_df = pd.DataFrame(matchings_rows, columns=cols)\n",
    "    \n",
    "    return match_df\n",
    "\n",
    "df_results = result_to_df(kpa_result)\n",
    "df_results.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = pd.DataFrame(sentences)\n",
    "df_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge results to original dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sentences = pd.read_csv(data + '/mtsamples_descriptions_clean.csv')\n",
    "#df_results['comment_id'] = df_results['comment_id'].astype(int)\n",
    "\n",
    "df_merge = df_results.merge(df_sentences[['id', 'id_description', 'medical_specialty_new']], left_on='comment_id', right_on='id', validate = 'one_to_one')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare results to distribution of medical specialties**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a preliminary idea of how big each cluster is\n",
    "df_merge['kp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure (figsize = (10,8))\n",
    "\n",
    "df_merge['kp'].value_counts().plot(kind = 'barh', color = '#ff00bf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kp in df_merge['kp'].value_counts().index:\n",
    "    df_merge[df_merge['kp'] == kp]['medical_specialty_new'].value_counts(normalize=True).plot(kind = 'bar')\n",
    "    plt.title('KP: ' + kp)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge['medical_specialty_new'].value_counts(normalize=True).plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Wikifier\n",
    "\n",
    "This service identifies the Wikipedia articles that are referenced by phrases or words or ideas, related to as mentions, in the sentence. For each such mention, the service returns several pieces of information, known together as the respective annotation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_to_mentions(sentences_texts):\n",
    "    term_wikifier_client = debater_api.get_term_wikifier_client()\n",
    "    mentions_list = term_wikifier_client.run(sentences_texts)\n",
    "    sentence_to_mentions = {}\n",
    "    for sentence_text, mentions in zip(sentences_texts,    \n",
    "                                       mentions_list):\n",
    "        sentence_to_mentions[sentence_text] = set([mention['concept']['title'] for mention in mentions])\n",
    "    \n",
    "    return sentence_to_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count Wikipedia terms in each key point\n",
    "from collections import Counter\n",
    "terms = {}\n",
    "for kp in set(df_merge['kp'].values):\n",
    "    sentence_to_mentions = get_sentence_to_mentions(df_merge['sentence_text'][df_merge['kp']==kp].values) # Extract Wikipedia terms\n",
    "    all_mentions = [mention for sentence in sentence_to_mentions for mention in sentence_to_mentions[sentence]] # Put terms in list\n",
    "    term_count = dict(Counter(all_mentions)) # Count terms and put in dictionary\n",
    "    if 'History' in term_count.keys():\n",
    "        term_count.pop('History')\n",
    "   \n",
    "    terms[kp] = term_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that it works\n",
    "pd.DataFrame(list(terms[' Fever, otitis media, and possible sepsis.'].items()),columns = ['Term','Count']).sort_values(by = 'Count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise\n",
    "for kp in df_merge['kp'].value_counts().index:\n",
    "    \n",
    "    _df_viz = pd.DataFrame(list(terms[kp].items()),columns = ['Term','Count']).sort_values(by = 'Count', ascending=True)\n",
    "    \n",
    "    fig = px.bar(x = _df_viz['Count'].tail(10),\n",
    "            y = _df_viz['Term'].tail(10),\n",
    "            color=_df_viz['Term'].tail(10),\n",
    "            color_discrete_sequence=px.colors.sequential.GnBu_r,\n",
    "            orientation = 'h',\n",
    "            title = 'Cluster:' + kp\n",
    "            )\n",
    "\n",
    "    fig.layout.update(showlegend = False, template = 'ggplot2', width = 700, height = 500,\n",
    "                yaxis = dict(title_text = 'Top 10 Wikipedia Terms',showline = True, showticklabels = True, color = 'black'),\n",
    "                xaxis = dict(title_text = 'Number of Mentions')\n",
    "                )\n",
    "\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  },
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
